{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu May 23 14:47:54 2019\n",
    "\n",
    "@author: Flemming Morsch\n",
    "\n",
    "Script for 1D convolutional network: Classification of compounds based on SMILES\n",
    "\"\"\"\n",
    "\n",
    "# General Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors\n",
    "from matplotlib import pyplot as plt \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "data = pd.read_excel('pyro_samples.xlsx', index_col=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split # not used for now, used split from paper\n",
    "\n",
    "#train-test split\n",
    "train = data[data.set == 'TR']\n",
    "X_train = train[['CanonicalSMILES']]\n",
    "y_train = to_categorical(train[['Stable_above_Tga']])\n",
    "\n",
    "test = data[data.set == 'TS']\n",
    "X_test = test[['CanonicalSMILES']]\n",
    "y_test = to_categorical(test[['Stable_above_Tga']])\n",
    "\n",
    "# extract all SMILES symbols from dataset, padding: start = '!' , end = 'E'\n",
    "charset = set(\"\".join(list(data.CanonicalSMILES))+\"!E\")\n",
    "char_to_int = dict((c,i) for i,c in enumerate(charset))\n",
    "int_to_char = dict((i,c) for i,c in enumerate(charset))\n",
    "embed = max([len(smile) for smile in data.CanonicalSMILES]) + 5\n",
    "print(str(charset))\n",
    "print(len(charset), embed)\n",
    "\n",
    "\n",
    "# define function to get one-hot encoded SMILES \n",
    "def vectorize(smiles):\n",
    "    '''\n",
    "    function to get one-hot encoded smiles vectors\n",
    "    https://www.wildcardconsulting.dk/master-your-molecule-generator-seq2seq-rnn-models-with-smiles-in-keras/ \n",
    "    \n",
    "    '''\n",
    "    one_hot =  np.zeros((smiles.shape[0], embed , len(charset)),dtype=np.int8)\n",
    "    for i,smile in enumerate(smiles):\n",
    "            #encode the startchar\n",
    "        one_hot[i,0,char_to_int[\"!\"]] = 0\n",
    "            #encode the rest of the chars\n",
    "        for j,c in enumerate(smile):\n",
    "            one_hot[i,j+1,char_to_int[c]] = 1\n",
    "            #Encode endchar\n",
    "            one_hot[i,len(smile)+1:,char_to_int[\"E\"]] = 0\n",
    "        #Return two, one for input and the other for output\n",
    "    return one_hot\n",
    "\n",
    "# get input samples for train and test\n",
    "X_train_smiles = vectorize(X_train.iloc[:,0]) \n",
    "X_test_smiles = vectorize(X_test.iloc[:,0])\n",
    "\n",
    "# see if the vectorize function works\n",
    "print(X_train.iloc[0])\n",
    "plt.matshow(X_train_smiles[0].T)\n",
    "# use this to get the smiles from the integers - set all values in vectorize to 1 to get the right structure !!!\n",
    "# remember to set it back to 0 before running the CNN !!!! \n",
    "\n",
    "smiles_16 = \"\".join([int_to_char[idx] for idx in np.argmax(X_train_smiles[16,:,:], axis=1)])\n",
    "# get chemical symbols of Chloramphenicol\n",
    "s_16 = []\n",
    "for i in smiles_16:\n",
    "    s_16 += i\n",
    "\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib as plt \n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, Conv1D,Conv2D, Flatten, MaxPool1D,MaxPool2D, GlobalAveragePooling2D, GlobalAveragePooling1D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras import optimizers\n",
    "# check if it is running on GPU !!!!\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus() # it does! - pytorch does not... \n",
    "##############################################################################################################\n",
    "# Instead of trainin the model again - just load it ! \n",
    "\n",
    "\n",
    "#model2 = load_model('conv1d_model_grant_class.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################\n",
    "# Specify the model\n",
    "model2 = Sequential()\n",
    "model2.add(Conv1D(100, kernel_size=9, activation='relu', strides=1, padding='same', \n",
    "                 input_shape=(103,24)))\n",
    "\n",
    "model2.add(MaxPool1D(pool_size=2, strides=1))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Conv1D(100, kernel_size=9, activation='relu', strides=1, padding='same'))\n",
    "model2.add(MaxPool1D(pool_size=2, strides=1))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(2, activation='softmax'))\n",
    "# Summarize the model \n",
    "model2.summary()\n",
    "\n",
    "# set up history class\n",
    "import keras\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "# Compile the model\n",
    "import time\n",
    "sgd = optimizers.SGD(lr=0.0005, clipvalue=0.5)\n",
    "import time # show the time needed for computing\n",
    "\n",
    "model2.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start = time.time()\n",
    "# Fit the model\n",
    "history = LossHistory()\n",
    "model2.fit(X_train_smiles, y_train, verbose=True, epochs=10000, validation_split=.20,  \n",
    "           callbacks=[history], batch_size=5)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "print('Time (min): ',time_taken/60)\n",
    "\n",
    "test_score, test_acc = model2.evaluate(X_test_smiles, y_test)\n",
    "train_score, train_acc = model2.evaluate(X_train_smiles, y_train)\n",
    "\n",
    "# print metrics\n",
    "print('Test score:', test_score)\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Train score:', train_score)\n",
    "print('Train accuracy:', train_acc)\n",
    "print(history.losses.index(min(history.losses)))\n",
    "\n",
    "# function to get binary values for predictions \n",
    "\n",
    "def make_binary(pred_binary):\n",
    "    ''' sets input to 1 or 0 with 0.5 threshold'''\n",
    "    for i in range(len(pred_binary)):\n",
    "        \n",
    "        if pred_binary[i] < 0.5:\n",
    "            pred_binary[i] = 0\n",
    "        else :\n",
    "            pred_binary[i] = 1\n",
    "        \n",
    "    return pred_binary\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# print classification reports \n",
    "test_pred = model2.predict(X_test_smiles)\n",
    "test_pred_binary = make_binary(test_pred[:,1])\n",
    "\n",
    "train_pred = model2.predict(X_train_smiles)\n",
    "train_pred_binary = make_binary(train_pred[:,1])\n",
    "\n",
    "print(confusion_matrix(y_test[:,1], test_pred_binary))\n",
    "print(classification_report(y_test[:,1], test_pred_binary))\n",
    "\n",
    "print(confusion_matrix(y_train[:,1], train_pred_binary))\n",
    "print(classification_report(y_train[:,1], train_pred_binary))\n",
    "\n",
    "# plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "#################################################################################################################\n",
    "# access first convolutional layer of Chloramphenicol \n",
    "# Get the first convolutional layer from the model\n",
    "c1 = model2.layers[0]\n",
    "# Get the weights of the first convolutional layer\n",
    "weights1 = c1.get_weights()\n",
    "# Pull out 24 kernels of shape 1 x 9 from filter 1 out of 100 \n",
    "kernel1 = weights1[0][:,:, 0]\n",
    "print(kernel1, kernel1.shape)\n",
    "\n",
    "\n",
    "def convolution1D(smiles, kernel):\n",
    "    '''\n",
    "    Function to convolute the SMILES with a 1D kernel \n",
    "    '''\n",
    "    conv = np.zeros(smiles.shape)\n",
    "    for ii in range(smiles.shape[0]-kernel.shape[0]+1):\n",
    "        conv[ii] = (kernel * smiles[ii:ii+kernel.shape[0]]).sum()\n",
    "    return conv\n",
    "\n",
    "# use the function 24 times for each of the channels to convolute Chloramphenicol\n",
    "result = np.zeros((103,24))\n",
    "for i in range(24):\n",
    "    result[:,i] = convolution1D(X_train_smiles[16,:,i], kernel1[:,i])\n",
    "    \n",
    "# plot Chloramphenicol: before and after the first convolution    \n",
    "x_pos = range(103)\n",
    "x_label = s_16\n",
    "\n",
    "# before CNN\n",
    "plt.matshow(X_train_smiles[16,:,:].T)\n",
    "plt.xticks(x_pos, x_label)\n",
    "plt.yticks(range(24), charset)\n",
    "\n",
    "# after first convolution\n",
    "plt.matshow(result.T)\n",
    "plt.xticks(x_pos, x_label)\n",
    "plt.yticks(range(24), charset)\n",
    "# get the SMILES of Chloramphenicol\n",
    "X_train.iloc[16,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
